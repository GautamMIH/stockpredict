# -*- coding: utf-8 -*-
"""lstmincreased

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1j1l0k8w5BP8ibHOK3fwf7l0aHgxhqB1i
"""

from google.colab import drive
drive.mount('/content/drive')

import tensorflow as tf
from sklearn.preprocessing import MinMaxScaler
import pandas as pd
import numpy as np

# Load your dataset
url = '/content/drive/MyDrive/content/NLICx.csv'
df = pd.read_csv(url)
df = df.applymap(lambda x: str(x).replace(',', ''))

# Input Features
features = ['Open', 'High', 'Low', 'Ltp', 'Qty', 'Turnover', '% Change', 'Inflation Rate']
df1 = df[features]

# Normalizing the data
scaler = MinMaxScaler(feature_range=(0, 1))
df_normalized = scaler.fit_transform(df1)

# Splitting the data into training and test sets
training_size = int(len(df_normalized) * 0.65)
test_size = len(df_normalized) - training_size
train_data, test_data = df_normalized[0:training_size, :], df_normalized[training_size:len(df_normalized), :]

#Create datasets for input and corresponding output
def create_dataset(dataset, time_step=1):
    dataX, dataY = [], []
    for i in range(len(dataset) - time_step - 1):
        a = dataset[i:(i + time_step), :]
        dataX.append(a)
        dataY.append(dataset[i + time_step, :])
    return np.array(dataX), np.array(dataY)

time_step = 100
X_train, y_train = create_dataset(train_data, time_step)
X_test, y_test = create_dataset(test_data, time_step)

# reshape the input data
X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], len(features))
X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], len(features))

#tanh and sigmoid functions
def sigmoid(x):
    return tf.nn.sigmoid(x)

def tanh(x):
    return tf.nn.tanh(x)


input_size = len(features) #initializing weights and varaiables for training
hidden_size = 50
output_size = len(features)

forget_gate_weights = tf.Variable(tf.random.normal([hidden_size, hidden_size + input_size]))
forget_gate_bias = tf.Variable(tf.zeros([1, hidden_size]))

input_gate_weights = tf.Variable(tf.random.normal([hidden_size, hidden_size + input_size]))
input_gate_bias = tf.Variable(tf.zeros([1, hidden_size]))

cell_gate_weights = tf.Variable(tf.random.normal([hidden_size, hidden_size + input_size]))
cell_gate_bias = tf.Variable(tf.zeros([1, hidden_size]))

output_gate_weights = tf.Variable(tf.random.normal([hidden_size, hidden_size + input_size]))
output_gate_bias = tf.Variable(tf.zeros([1, hidden_size]))

weights_output = tf.Variable(tf.random.normal([hidden_size, output_size]))
bias_output = tf.Variable(tf.zeros([1, output_size]))


learning_rate = 0.001 #hyperparameters for training
epochs = 10

optimizer = tf.optimizers.Adam(learning_rate)

# Training the LSTM
for epoch in range(epochs):

    for i in range(len(X_train)):


        # Forward pass
        inputs = X_train[i]
        targets = y_train[i]

        hidden_state = tf.zeros((1, hidden_size))
        cell_state = tf.zeros((1, hidden_size))

        loss = 0

        with tf.GradientTape() as tape:
            for t in range(time_step):
                combined = tf.concat([tf.cast(hidden_state, dtype=tf.float32), tf.cast(tf.expand_dims(inputs[t], axis=0), dtype=tf.float32)], axis=1)
                forget_gate = sigmoid(tf.matmul(combined, forget_gate_weights, transpose_b=True) + forget_gate_bias)
                input_gate = sigmoid(tf.matmul(combined, input_gate_weights, transpose_b=True) + input_gate_bias)
                cell_gate = tanh(tf.matmul(combined, cell_gate_weights, transpose_b=True) + cell_gate_bias)
                output_gate = sigmoid(tf.matmul(combined, output_gate_weights, transpose_b=True) + output_gate_bias)
                cell_state = forget_gate * cell_state + input_gate * cell_gate
                hidden_state = output_gate * tanh(cell_state)

            # Output layer
            output = sigmoid(tf.matmul(hidden_state, weights_output) + bias_output)


            # calculate loss
            loss += tf.reduce_sum(tf.square(output - targets))

        # Backward pass
        gradients = tape.gradient(loss, [forget_gate_weights, forget_gate_bias,
                                         input_gate_weights, input_gate_bias,
                                         cell_gate_weights, cell_gate_bias,
                                         output_gate_weights, output_gate_bias,
                                         weights_output, bias_output])

        # Updating weights and biases using the optimizer
        optimizer.apply_gradients(zip(gradients, [forget_gate_weights, forget_gate_bias,
                                                 input_gate_weights, input_gate_bias,
                                                 cell_gate_weights, cell_gate_bias,
                                                 output_gate_weights, output_gate_bias,
                                                 weights_output, bias_output]))

np.savetxt("weights_forget_gate.csv", forget_gate_weights, delimiter=",")
np.savetxt("bias_forget_gate.csv", forget_gate_bias, delimiter=",")
np.savetxt("weights_input_gate.csv", input_gate_weights, delimiter=",")
np.savetxt("bias_input_gate.csv", input_gate_bias, delimiter=",")
np.savetxt("weights_cell_gate.csv", cell_gate_weights, delimiter=",")
np.savetxt("bias_cell_gate.csv", cell_gate_bias, delimiter=",")
np.savetxt("weights_output_gate.csv", output_gate_weights, delimiter=",")
np.savetxt("bias_output_gate.csv", output_gate_bias, delimiter=",")
np.savetxt("weights_output.csv", weights_output, delimiter=",")
np.savetxt("bias_output.csv", bias_output, delimiter=",")

print("Training complete. Weights and biases saved.")

forget_gate_weights = np.loadtxt("weights_forget_gate.csv", delimiter=",")
forget_gate_bias = np.loadtxt("bias_forget_gate.csv", delimiter=",")
input_gate_weights = np.loadtxt("weights_input_gate.csv", delimiter=",")
input_gate_bias = np.loadtxt("bias_input_gate.csv", delimiter=",")
cell_gate_weights = np.loadtxt("weights_cell_gate.csv", delimiter=",")
cell_gate_bias = np.loadtxt("bias_cell_gate.csv", delimiter=",")
output_gate_weights = np.loadtxt("weights_output_gate.csv", delimiter=",")
output_gate_bias = np.loadtxt("bias_output_gate.csv", delimiter=",")
weights_output = np.loadtxt("weights_output.csv", delimiter=",")
bias_output = np.loadtxt("bias_output.csv", delimiter=",")


url = '/content/drive/MyDrive/content/NLICx.csv'
input_df = pd.read_csv(url)
input_df = input_df.applymap(lambda x: str(x).replace(',', ''))

input_features = ['Open', 'High', 'Low', 'Ltp', 'Qty', 'Turnover', '% Change', 'Inflation Rate']
input_data = input_df[input_features]
input_normalized = scaler.transform(input_data)

input_sequence = np.array([input_normalized[-time_step:, :]])
input_sequence = input_sequence.reshape(1, time_step, len(features))

hidden_state = np.zeros((1, hidden_size))
predicted_values = []

for t in range(time_step):

    combined = np.hstack((np.squeeze(input_sequence[0, t]), np.reshape(hidden_state, (hidden_size,))))
    forget_gate = sigmoid(np.dot(combined, forget_gate_weights.T) + forget_gate_bias)
    input_gate = sigmoid(np.dot(combined, input_gate_weights.T) + input_gate_bias)
    cell_gate = tanh(np.dot(combined, cell_gate_weights.T) + cell_gate_bias)
    output_gate = sigmoid(np.dot(combined, output_gate_weights.T) + output_gate_bias)

    cell_state = tf.cast(forget_gate, dtype=tf.float64) * tf.cast(cell_state, dtype=tf.float64) + tf.cast(input_gate, dtype=tf.float64) * tf.cast(cell_gate, dtype=tf.float64)
    hidden_state = output_gate * tanh(cell_state)

    # Output layer
    output = sigmoid(np.dot(hidden_state, weights_output) + bias_output)
    predicted_values.append(output)


predicted_values = np.array(predicted_values).reshape(-1, len(features))
predicted_values = scaler.inverse_transform(predicted_values)


predicted_df = pd.DataFrame(predicted_values, columns=features)
print("Predicted Values for the Next Day:")
print(predicted_df)

hidden_state = np.zeros((1, hidden_size))
predicted_values = np.zeros((1, len(features)))


for t in range(time_step):
    combined = np.hstack((np.squeeze(input_sequence[0, t]), np.reshape(hidden_state, (hidden_size,))))
    forget_gate = sigmoid(np.dot(combined, forget_gate_weights.T) + forget_gate_bias)
    input_gate = sigmoid(np.dot(combined, input_gate_weights.T) + input_gate_bias)
    cell_gate = tanh(np.dot(combined, cell_gate_weights.T) + cell_gate_bias)
    output_gate = sigmoid(np.dot(combined, output_gate_weights.T) + output_gate_bias)

    cell_state = forget_gate * cell_state + input_gate * cell_gate
    hidden_state = output_gate * tanh(cell_state)

    # Output layer
    output = sigmoid(np.dot(hidden_state, weights_output) + bias_output)

output = scaler.inverse_transform(output.numpy().reshape(1, -1))

predicted_df = pd.DataFrame(output, columns=features)
print("Predicted Values for the Next Day:")
print(predicted_df)

import sqlite3
# After obtaining the final predicted output
final_output = output.reshape(1, -1)

database_path = '/content/drive/MyDrive/content/predictions.db'
connection = sqlite3.connect(database_path)

# Create a table if it doesn't exist
cursor = connection.cursor()
cursor.execute('''
    CREATE TABLE IF NOT EXISTS predictions (
        SYMBOL STR,
        Open REAL,
        High REAL,
        Low REAL,
        Ltp REAL,
        Qty REAL,
        Turnover REAL,
        PercentChange REAL,
        InflationRate REAL
    )
''')

# Insert the final predicted output into the database
cursor.execute('''
    INSERT INTO predictions VALUES ('NLIC',?, ?, ?, ?, ?, ?, ?, ?)
''', tuple(final_output.flatten()))

# Commit the changes and close the connection
connection.commit()
connection.close()

print("Last predicted output saved to the database.")