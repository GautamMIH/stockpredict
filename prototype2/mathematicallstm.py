# -*- coding: utf-8 -*-
"""mathematicallstm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bqlQmvedZoD71Iub-HMXlC6L_otH4ICQ
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
from sklearn.preprocessing import MinMaxScaler
import numpy as np

# Load your dataset
url = '/content/drive/MyDrive/content/NLICx.csv'
df = pd.read_csv(url)
df = df.applymap(lambda x: str(x).replace(',', ''))

# Consider multiple features
features = ['Open', 'High', 'Low', 'Ltp', 'Qty', 'Turnover','% Change', 'Inflation Rate']
df1 = df[features]

# Normalize the data
scaler = MinMaxScaler(feature_range=(0, 1))
df_normalized = scaler.fit_transform(df1)

# Split the data into training and test sets
training_size = int(len(df_normalized) * 0.65)
test_size = len(df_normalized) - training_size
train_data, test_data = df_normalized[0:training_size, :], df_normalized[training_size:len(df_normalized), :]

# Function to create dataset with multiple features
def create_dataset(dataset, time_step=1):
    dataX, dataY = [], []
    for i in range(len(dataset) - time_step - 1):
        a = dataset[i:(i + time_step), :]
        dataX.append(a)
        dataY.append(dataset[i + time_step, :])
    return np.array(dataX), np.array(dataY)

time_step = 100
X_train, y_train = create_dataset(train_data, time_step)
X_test, y_test = create_dataset(test_data, time_step)

# Reshape the input data
X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], len(features))
X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], len(features))

import numpy as np
# Define the sigmoid and tanh activation functions
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def tanh(x):
    return np.tanh(x)

# Initialize weights and biases
input_size = len(features)
hidden_size = 50
output_size = len(features)

weights_input = np.random.randn(input_size, hidden_size)
weights_hidden = np.random.randn(hidden_size, hidden_size)
weights_output = np.random.randn(hidden_size, output_size)

bias_hidden = np.zeros((1, hidden_size))
bias_output = np.zeros((1, output_size))

# Training hyperparameters
learning_rate = 0.001
epochs = 100

# Training the LSTM
for epoch in range(epochs):
    for i in range(len(X_train)):
        # Forward pass
        inputs = X_train[i]
        targets = y_train[i]

        hidden_state = np.zeros((1, hidden_size))
        cell_state = np.zeros((1, hidden_size))

        loss = 0

        for t in range(time_step):
            # Update hidden state
            combined = np.dot(inputs[t], weights_input) + np.dot(hidden_state, weights_hidden) + bias_hidden
            hidden_state = np.tanh(combined)

        # Output layer
        output = np.dot(hidden_state, weights_output) + bias_output

        # Backward pass
        output_error = output - targets
        loss += np.sum(output_error ** 2)

        # Update weights and biases
        weights_output -= learning_rate * np.dot(hidden_state.T, output_error)
        hidden_error = np.dot(output_error, weights_output.T)
        hidden_error = hidden_error * (1 - hidden_state ** 2)

        for t in reversed(range(time_step)):
            # Update hidden state
            combined = np.dot(inputs[t], weights_input) + np.dot(hidden_state, weights_hidden) + bias_hidden
            hidden_state = np.tanh(combined)

            # Update weights and biases
            weights_input -= learning_rate * np.dot(inputs[t].reshape(-1, 1), hidden_error)
            weights_hidden -= learning_rate * np.dot(hidden_state.T, hidden_error)
            bias_hidden -= learning_rate * hidden_error

# Make predictions for the next day
last_day_data = df_normalized[-time_step:, :]
hidden_state = np.zeros((1, hidden_size))
cell_state = np.zeros((1, hidden_size))

predicted_values = []

for t in range(time_step):
    # Update hidden state
    combined = np.dot(last_day_data[t], weights_input) + np.dot(hidden_state, weights_hidden) + bias_hidden
    hidden_state = tanh(combined)

# Output layer
output = np.dot(hidden_state, weights_output) + bias_output
predicted_values.append(output)

# Inverse transform the predicted values
predicted_values = np.array(predicted_values).reshape(-1, len(features))
predicted_values = scaler.inverse_transform(predicted_values)

# Print the predicted values
predicted_df = pd.DataFrame(predicted_values, columns=features)
print("Predicted Values for the Next Day:")
print(predicted_df)

import numpy as np
import pandas as pd


# Save the trained weights and biases to separate CSV files
np.savetxt("weights_input.csv", weights_input, delimiter=",")
np.savetxt("weights_hidden.csv", weights_hidden, delimiter=",")
np.savetxt("weights_output.csv", weights_output, delimiter=",")
np.savetxt("bias_hidden.csv", bias_hidden, delimiter=",")
np.savetxt("bias_output.csv", bias_output, delimiter=",")

# Print a message indicating that training is complete
print("Training complete. Weights and biases saved.")

import pandas as pd
import numpy as np

input_size = len(features)
hidden_size = 50
output_size = len(features)
# # Load the saved weights from the CSV file
# saved_weights = np.loadtxt("lstm_weights.csv", delimiter=",")

# Load the saved weights and biases from separate CSV files
weights_input = np.loadtxt("weights_input.csv", delimiter=",")
weights_hidden = np.loadtxt("weights_hidden.csv", delimiter=",")
weights_output = np.loadtxt("weights_output.csv", delimiter=",")
bias_hidden = np.loadtxt("bias_hidden.csv", delimiter=",")
bias_output = np.loadtxt("bias_output.csv", delimiter=",")


# Load input data for prediction
# Replace this with your own input data
url = '/content/drive/MyDrive/content/NLICx.csv'
input_df = pd.read_csv(url)
input_df = input_df.applymap(lambda x: str(x).replace(',', ''))

# Consider the same features used during training
input_features = ['Open', 'High', 'Low', 'Ltp', 'Qty', 'Turnover', '% Change', 'Inflation Rate']
input_data = input_df[input_features]

# Normalize the input data using the same scaler used during training
input_normalized = scaler.transform(input_data)

# Reshape the input data to match the model's expected input shape
input_sequence = np.array([input_normalized[-time_step:, :]])
input_sequence = input_sequence.reshape(1, time_step, len(features))

# Make predictions
hidden_state = np.zeros((1, hidden_size))
predicted_values = []

for t in range(time_step):
    # Update hidden state
    combined = np.dot(input_sequence[0, t], weights_input) + np.dot(hidden_state, weights_hidden) + bias_hidden
    hidden_state = np.tanh(combined)

# Output layer
output = np.dot(hidden_state, weights_output) + bias_output
predicted_values.append(output)

# Inverse transform the predicted values
predicted_values = np.array(predicted_values).reshape(-1, len(features))
predicted_values = scaler.inverse_transform(predicted_values)

# Print the predicted values
predicted_df = pd.DataFrame(predicted_values, columns=features)
print("Predicted Values for the Next Day:")
print(predicted_df)

from sklearn.metrics import mean_squared_error, r2_score
# Make predictions on the test data
test_predictions = []

for i in range(len(X_test)):
    hidden_state = np.zeros((1, hidden_size))

    for t in range(time_step):
        combined = np.dot(X_test[i, t], weights_input) + np.dot(hidden_state, weights_hidden) + bias_hidden
        hidden_state = np.tanh(combined)

    output = np.dot(hidden_state, weights_output) + bias_output
    test_predictions.append(output)

# Inverse transform the predicted values
test_predictions = np.array(test_predictions).reshape(-1, len(features))
test_predictions = scaler.inverse_transform(test_predictions)

# Inverse transform the actual values
y_test_actual = scaler.inverse_transform(y_test)

# Calculate accuracy metrics
mse = mean_squared_error(y_test_actual, test_predictions)
r2 = r2_score(y_test_actual, test_predictions)


print("Mean Squared Error on Test Data:", mse)
print("R-squared on Test Data:", r2)


from sklearn.metrics import mean_squared_error
from math import sqrt

# Inverse transform the true values for the test set


# Inverse transform the predicted values for the test set




# Calculate MAPE
mape = np.mean(np.abs((y_test_actual - test_predictions) / y_test_actual)) * 100
print(f'MAPE: {mape}%')

# Set a threshold percentage for accuracy calculation
threshold_percentage = 10

# Calculate the absolute percentage error for each prediction
absolute_percentage_error = np.abs((test_predictions - y_test_actual) / y_test_actual) * 100

# Count the number of predictions within the threshold
correct_predictions = np.sum(absolute_percentage_error <= threshold_percentage)

# Calculate the percentage of correct predictions
accuracy_percentage = (correct_predictions / len(y_test_actual)) * 100

# Print the accuracy as a percentage
print(f"Accuracy on Test Data (within): {accuracy_percentage:.2f}%")

#Make predictions for the next day
last_day_data = df_normalized[-time_step:, :]
hidden_state = np.zeros((1, hidden_size))

predicted_values = []

for t in range(time_step):
    # Update hidden state
    combined = np.dot(last_day_data[t], weights_input) + np.dot(hidden_state, weights_hidden) + bias_hidden
    hidden_state = tanh(combined)

# Output layer
output = np.dot(hidden_state, weights_output) + bias_output

# Apply the threshold for classification
predicted_class = "Increase" if output[0, 0] > last_day_data[-1, features.index('Ltp')] else "Decrease"

# Print the predicted class
print("Predicted Class for the Next Day:")
print(predicted_class)

import pandas as pd
import numpy as np
import math
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM
from sklearn.preprocessing import MinMaxScaler
import math
from sklearn.metrics import mean_squared_error
from tensorflow.keras.callbacks import ModelCheckpoint

# Load your dataset
url = '/content/drive/MyDrive/content/NLICx.csv'
df = pd.read_csv(url)
df = df.applymap(lambda x: str(x).replace(',', ''))

# Convert columns to numeric, handling errors
numeric_columns = ['Open', 'High', 'Low', 'Ltp', 'Qty', 'Turnover','Inflation Rate' ] #'Inflation Rate','% Change'
df[numeric_columns] = df[numeric_columns].apply(pd.to_numeric, errors='coerce')

# Drop rows with missing values
df = df.dropna()

# Consider multiple features
features = numeric_columns
df1 = df[features]

# Calculate IQR for each feature
Q1 = df1.quantile(0.25)
Q3 = df1.quantile(0.75)
IQR = Q3 - Q1

# Define a threshold for outlier detection (you can adjust this threshold)
threshold = 1.5

# Identify outliers based on the IQR
outliers = ((df1 < (Q1 - threshold * IQR)) | (df1 > (Q3 + threshold * IQR))).any(axis=1)

# Remove rows with outliers
df1_no_outliers = df1[~outliers]
# Normalize the data
scaler = MinMaxScaler(feature_range=(0, 1))
df_normalized = scaler.fit_transform(df1)

# Split the data into training and test sets
training_size = int(len(df_normalized) * 0.8)
test_size = len(df_normalized) - training_size
train_data, test_data = df_normalized[0:training_size, :], df_normalized[training_size:len(df_normalized), :]

# Function to create dataset with multiple features
def create_dataset(dataset, time_step=1):
    dataX, dataY = [], []
    for i in range(len(dataset) - time_step - 1):
        a = dataset[i:(i + time_step), :]
        dataX.append(a)
        dataY.append(dataset[i + time_step, :])
    return np.array(dataX), np.array(dataY)

time_step = 100
X_train, y_train = create_dataset(train_data, time_step)
X_test, y_test = create_dataset(test_data, time_step)

# Reshape the input data
X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], len(features))
X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], len(features))

# Build the LSTM model
model = Sequential()
model.add(LSTM(50, return_sequences=True, input_shape=(time_step, len(features))))
model.add(LSTM(50, return_sequences=True))
model.add(LSTM(50))
model.add(Dense(len(features), activation='linear'))  # Adjust the number of neurons in the Dense layer based on the number of features
model.compile(loss='mean_squared_error', optimizer='adam')

# Set up model checkpoint to save weights at each epoch
checkpoint_path = "/content/drive/MyDrive/content/weightsnlic/weights_epoch_{epoch:02d}.h5"
checkpoint = ModelCheckpoint(checkpoint_path, save_weights_only=True, period=1)


# Train the model
model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=64, verbose=1, callbacks=[checkpoint])

# Make predictions
test_predict = model.predict(X_test)

# Inverse transform the predictions
test_predict = scaler.inverse_transform(test_predict)

# Plot the results
look_back = 100

# Plot the data
import matplotlib.pyplot as plt

for feature_idx in range(len(features)):
    testPredictPlot = np.empty_like(df_normalized)
    testPredictPlot[:, :] = np.nan
    testPredictPlot[len(y_train) + (look_back * 2) + 1:len(df_normalized) - 1, feature_idx] = test_predict[:, feature_idx]

    plt.figure(figsize=(10, 5))
    plt.plot(scaler.inverse_transform(df_normalized)[:, feature_idx], label='Actual')
    plt.plot(testPredictPlot[:, feature_idx], label='Predicted')
    plt.title(f'{features[feature_idx]} Prediction')
    plt.legend()
    plt.show()

from sklearn.metrics import mean_squared_error
from math import sqrt

# Inverse transform the true values for the test set
y_test_actual = scaler.inverse_transform(y_test)

# Inverse transform the predicted values for the test set
y_test_pred = model.predict(X_test)
y_test_pred = scaler.inverse_transform(y_test_pred)


# Calculate MAPE
mape = np.mean(np.abs((y_test_actual - y_test_pred) / y_test_actual)) * 100
print(f'MAPE: {mape}%')

# Make predictions for the next day
last_day_data = df_normalized[-time_step:, :]
last_day_data = last_day_data.reshape(1, time_step, len(features))
predicted_values = model.predict(last_day_data)

# Inverse transform the predicted values
predicted_values = scaler.inverse_transform(predicted_values)

# Print the predicted values
predicted_df = pd.DataFrame(predicted_values, columns=features)
print("Predicted Values for the Next Day:")
print(predicted_df)

# Calculate RMSE for each feature
rmse_list = []
for feature_idx in range(len(features)):
    mse = mean_squared_error(y_test[:, feature_idx], test_predict[:, feature_idx])
    rmse = np.sqrt(mse)
    rmse_list.append(rmse)
    print(f'RMSE for {features[feature_idx]}: {rmse}')

# Calculate overall RMSE (average of individual feature RMSEs)
overall_rmse = np.mean(rmse_list)
print(f'Overall RMSE: {overall_rmse}')